{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from particle_dataset import ParticleDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "WINDOW_SIZE = 16\n",
    "\n",
    "TARGET_LABEL = 'PM2.5'\n",
    "FEATURE_LABEL = [\n",
    "    'PM1_H_OUT',\n",
    "    'PM2.5_H_OUT',\n",
    "    'PM10_H_OUT',\n",
    "    'PM1_OUT',\n",
    "    'PM2.5_OUT',\n",
    "    'PM10_OUT',\n",
    "    'PERSON_NUMBER',\n",
    "    'AIR_PURIFIER',\n",
    "    'WINDOW'\n",
    "]\n",
    "\n",
    "DATASET_PATH = '../../datasets/summary/particles_inout.csv'\n",
    "WEIGHT_PATH = '../model_weights/lstm_v3_weights.h5'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.read_csv('../../datasets/summary/particles_inout.csv').describe().transpose()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataset and remove rows where pm2.5 is over 150\n",
    "print('Loading dataset...')\n",
    "df_org = pd.read_csv(DATASET_PATH)\n",
    "df_org['DATE'] = df_org['DATE'].apply(pd.to_datetime)\n",
    "df_org.drop(df_org[df_org['PM2.5'] > 150].index, inplace=True)\n",
    "print('Successfully loaded!')\n",
    "print('Original dataset shape: ', df_org.shape)\n",
    "\n",
    "print(f'Remove date column and calculate moving average with window size {WINDOW_SIZE}')\n",
    "df = df_org.drop(columns=['DATE'])\n",
    "df = df.rolling(window=10).mean()\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop='index', inplace=True)\n",
    "print('Trimed dataset shape: ', df.shape)\n",
    "\n",
    "y_df = df[TARGET_LABEL]\n",
    "x_df = df[FEATURE_LABEL]\n",
    "\n",
    "# Data Normalization for PM features (사람 수, 공기청정기, 창문 데이터는 해당되지 않음)\n",
    "cols = ['PM1_H_OUT', 'PM2.5_H_OUT', 'PM10_H_OUT', 'PM1_OUT', 'PM2.5_OUT', 'PM10_OUT']\n",
    "for col in cols:\n",
    "    x_df[col] = (x_df[col] - x_df[col].mean()) / x_df[col].std()\n",
    "\n",
    "x_df.reset_index(drop='index', inplace=True)\n",
    "y_df.reset_index(drop='index', inplace=True)\n",
    "\n",
    "data_size = x_df.shape[0] - WINDOW_SIZE\n",
    "\n",
    "X = np.zeros((data_size, WINDOW_SIZE, x_df.shape[1]))\n",
    "for i in range(data_size):\n",
    "    X[i] = x_df[i:i + WINDOW_SIZE].values\n",
    "\n",
    "y = y_df.loc[WINDOW_SIZE:].values\n",
    "\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_test_split_with_sequence(_X, _y, test_size=0.25):\n",
    "    size = _X.shape[0]\n",
    "    split_index = int(np.ceil(size * (1 - test_size)))\n",
    "    return _X[:split_index], _X[split_index:], _y[:split_index], _y[split_index:]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_with_sequence(X, y, test_size=0.25)\n",
    "X_train, X_val, y_train, y_val = train_test_split_with_sequence(X_train, y_train, test_size=0.2)\n",
    "\n",
    "print('X_train, y_train shape: ', X_train.shape, y_train.shape)\n",
    "print('X_val, y_val shape: ', X_val.shape, y_val.shape)\n",
    "print('X_test, y_test shape: ', X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds = ParticleDS(X_train, y_train, window_size=WINDOW_SIZE, batch_size=BATCH_SIZE)\n",
    "val_ds = ParticleDS(X_val, y_val, window_size=WINDOW_SIZE, batch_size=BATCH_SIZE)\n",
    "test_ds = ParticleDS(X_test, y_test, window_size=WINDOW_SIZE, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Dropout, AveragePooling1D, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "input_tensor = Input(shape=(WINDOW_SIZE, 9), name='input')\n",
    "\n",
    "x = Conv1D(filters=128, kernel_size=3, strides=1, padding='same', name='conv1d_1')(input_tensor)\n",
    "x = Conv1D(filters=64, kernel_size=3, strides=1, padding='same', name='conv1d_2')(x)\n",
    "x = AveragePooling1D(pool_size=3, strides=1, padding='same', name='pooling_1')(x)\n",
    "\n",
    "x = LSTM(units=12, return_sequences=True, name='lstm_1')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = LSTM(units=32, return_sequences=True, name='lstm_2')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(units=16, return_sequences=False, name='lstm_3')(x)\n",
    "x = Dense(128, activation='relu', name='fc_1')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', name='fc_2')(x)\n",
    "output = Dense(1, name='output')(x)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output, name='lstm_v3')\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Metric으로 사용할 목적으로 작성했지만, 결과가 일반 MSE와 크게 다르지 않아 사용하지 않음\n",
    "def last_time_step_mse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true[:-1], y_pred[:-1])\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error', metrics=[last_time_step_mse])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 모델 학습 부분, 실제 학습하는 과정을 보려면 아래 주석 코드를 사용하면 됩니다.\n",
    "# CPU 환경에서는 시간이 많이 소요되어 Kaggle 혹은 Colab GPU 환경을 사용하시는 것을 추천드립니다.\n",
    "\n",
    "\"\"\"\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='min', verbose=1)\n",
    "ely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath='/content/drive/MyDrive/cpfd/models/weights/lstm_v3_weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True, save_weights_only=True, mode='min', period=1, verbose=0)\n",
    "\n",
    "history = model.fit(train_ds, epochs=30, validation_data=val_ds, callbacks=[rlr_cb, ely_cb, mcp_cb])\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='valid')\n",
    "plt.legend()\n",
    "\"\"\"\n",
    "\n",
    "# 학습하는 것이 아니라 학습된 모델을 사용할 경우, 아래 코드를 사용해주시기 바랍니다.\n",
    "model.load_weights(WEIGHT_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking MSE for test dataset\n",
    "model.evaluate(test_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create result dataframe with Real and Predict data\n",
    "pred = model.predict(test_ds, batch_size=32)\n",
    "result = pd.DataFrame({'Real': y_test, 'Pred': pred.reshape(len(pred))})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = result.plot(kind='line', figsize=(36, 10), fontsize=17)\n",
    "ax.set_xlabel('TIME', fontsize=24)\n",
    "ax.set_ylabel('PM 2.5', fontsize=24)\n",
    "ax.legend(fontsize=30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "result_under_100 = result[result['Real'] < 80]\n",
    "r2 = r2_score(result_under_100['Real'].values, result_under_100['Pred'].values)\n",
    "print('R Square: %.4f' % r2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}